{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "### 1. 데이터 처음 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `python src/KoBART/train.py` 실행\n",
    "\n",
    "\n",
    "2. 입력값   \n",
    "    - **train_file**   \n",
    "    `train 데이터 주소` 입력\n",
    "    - **test_file**   \n",
    "    `test 데이터 주소` 입력\n",
    "    - **mode**  \n",
    "    `train` 입력\n",
    "    - **batch_size**   \n",
    "    `batch size` (ex.16) 입력  \n",
    "    auto_scale_batch_size='power' 등을 설정하더라도 임의의 값을 입력해야 함\n",
    "    - **num_workers**  \n",
    "    `num_workers` (ex.10) 입력   \n",
    "    가상 cpu 코어 개수 : 72  \n",
    "    물리적 코어 개수 = 가상 cpu 코어 개수의 절반 = 36   \n",
    "    적절한 num workers = 물리적 코어 개수의 절반 = 18  \n",
    "    num workers 18로 진행시 -> Bus error (out of shared memory) -> 도커의 shared memory 변경하면 오류 해결 가능할 것으로 예상\n",
    "    - **gradient_clip_val**   \n",
    "    `default 값`인 1.0 입력    \n",
    "    - **gpus**   \n",
    "    `제공되는 gpu의 개수` (ex.2) 인 입력\n",
    "    - **accelerator**   \n",
    "    multi gpu 사용이므로`ddp` 입력  \n",
    "    - **max_epochs**  \n",
    "    `최대 epoch 수` (ex.50) 입력\n",
    "    - **default_root_dir**  \n",
    "    `학습 결과를 저장할 폴더 경로` 입력\n",
    "    \n",
    "    \n",
    "3. 저장된 학습 결과 확인  \n",
    "    **default_root_dir** 폴더 안에 생성되는 폴더 및 파일들\n",
    "    - **kobart_summary-model_chp**   \n",
    "        : `epoch=xx-val_loss=x.xxx.ckpt` 의 형태로 체크포인트 파일 생성\n",
    "    - **tb_logs > default**   \n",
    "        : `version_x` 의 형태로 버전 폴더 생성   \n",
    "        : 버전 폴더 안 `events.out.tfevents.xxxxxxxxxxxx`, `hparams.yaml` 의 형태로 이벤트 파일과 하이퍼파라미터 파일 생성\n",
    "    - **kobart_summary-last.ckpt**   \n",
    "        : 가장 마지막 체크포인트 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=16, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path=None, default_root_dir='rsc/KoBART-AIHub/aihub한국어대화요약2', deterministic=False, distributed_backend=None, enable_pl_optimizer=True, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=2, gradient_clip_val=1.0, hparams_file=None, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=50, max_len=512, max_steps=None, min_epochs=1, min_steps=None, mode='train', model_path=None, move_metrics_to_cpu=False, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=10, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sync_batchnorm=False, terminate_on_nan=False, test_file='rsc/data/Validation/AI_hub/한국어대화요약/dev_v1.csv', tpu_cores=<function _gpus_arg_default at 0x7f239368b158>, track_grad_norm=-1, train_file='rsc/data/Training/AI_hub/한국어대화요약/train_v1.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
      "using cached model\n",
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "using cached model\n",
      "using cached model\n",
      "Missing logger folder: rsc/KoBART-AIHub/aihub한국어대화요약2/tb_logs/default\n",
      "WARNING:lightning:Missing logger folder: rsc/KoBART-AIHub/aihub한국어대화요약2/tb_logs/default\n",
      "INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=16, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path=None, default_root_dir='rsc/KoBART-AIHub/aihub한국어대화요약2', deterministic=False, distributed_backend=None, enable_pl_optimizer=True, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=2, gradient_clip_val=1.0, hparams_file=None, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=50, max_len=512, max_steps=None, min_epochs=1, min_steps=None, mode='train', model_path=None, move_metrics_to_cpu=False, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=10, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sync_batchnorm=False, terminate_on_nan=False, test_file='rsc/data/Validation/AI_hub/한국어대화요약/dev_v1.csv', tpu_cores=<function _gpus_arg_default at 0x7fcba19c2048>, track_grad_norm=-1, train_file='rsc/data/Training/AI_hub/한국어대화요약/train_v1.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
      "using cached model\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "INFO:lightning:initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "using cached model\n",
      "using cached model\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "INFO:lightning:initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "INFO:root:number of workers 10, data length 280000\n",
      "INFO:root:num_train_steps : 87500\n",
      "INFO:root:num_warmup_steps : 8750\n",
      "INFO:root:number of workers 10, data length 280000\n",
      "INFO:root:num_train_steps : 87500\n",
      "INFO:root:num_warmup_steps : 8750\n",
      "2021-08-25 08:53:50.581852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 123 M \n",
      "-------------------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "INFO:lightning:\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 123 M \n",
      "-------------------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "Epoch 0:  10%| | 1003/9844 [08:24<1:14:08,  1.99it/s, loss=3.01, v_num=0, val_lo"
     ]
    }
   ],
   "source": [
    "!python src/KoBART/train.py \\\n",
    "--train_file='rsc/Data/Training/AI_hub/한국어대화요약/train_v1.csv' \\\n",
    "--test_file='rsc/Data/Validation/AI_hub/한국어대화요약/dev_v1.csv' \\\n",
    "--mode='train' \\\n",
    "--batch_size=16 \\\n",
    "--num_workers=10 \\\n",
    "--gradient_clip_val=1.0 \\\n",
    "--gpus=2 \\\n",
    "--accelerator=ddp \\\n",
    "--max_epochs=50 \\\n",
    "--default_root_dir='rsc/By_domain_ckpt/aihub한국어대화요약'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 이어 학습하기 (체크포인트 이어 학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `python src/KoBART/train.py` 실행\n",
    "\n",
    "\n",
    "2. 입력값\n",
    "    - **checkpoint_path**    \n",
    "    학습한 모델의 체크포인트 중 val_loss 가 가장 낮은 `체크포인트 파일 경로` 입력\n",
    "    - **hparams_file**   \n",
    "    학습한 모델의 `하이퍼파라미터 파일 경로` 입력\n",
    "    - 나머지는 위와 동일\n",
    "    \n",
    "    \n",
    "3. 저장된 학습 결과 확인   \n",
    "    위와 동일 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/KoBART/train.py \\\n",
    "--train_file='rsc/Data/Training/AI_hub/한국어대화요약/train_v1.csv' \\\n",
    "--test_file='rsc/Data/Validation/AI_hub/한국어대화요약/dev_v1.csv' \\\n",
    "--checkpoint_path='rsc/By_domain_ckpt/dacon신문기사/kobart_summary-model_chp/epoch=02-val_loss=1.326.ckpt' \\\n",
    "--hparams_file='rsc/By_domain_ckpt/dacon신문기사/tb_logs/default/version_1/hparams.yaml' \\\n",
    "--mode='train' \\\n",
    "--batch_size=16 \\\n",
    "--num_workers=10 \\\n",
    "--gradient_clip_val=1.0 \\\n",
    "--gpus=2 \\\n",
    "--accelerator=ddp \\\n",
    "--max_epochs=50 \\\n",
    "--default_root_dir='rsc/By_domain_ckpt/aihub한국어대화요약_dacon신문기사'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "### 학습한 모델로 테스트하기 & rouge 값 출력하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `python src/KoBART/train.py` 실행\n",
    "\n",
    "\n",
    "2. 입력값   \n",
    "    - **test_file**   \n",
    "    `test 데이터 주소` 입력\n",
    "    - **mode**  \n",
    "    `test` 입력\n",
    "    - **checkpoint_path**    \n",
    "    학습한 모델의 체크포인트 중 val_loss 가 가장 낮은 `체크포인트 파일 경로` 입력\n",
    "    - **hparams_file**   \n",
    "    학습한 모델의 `하이퍼파라미터 파일 경로` 입력\n",
    "    - **default_root_dir**  \n",
    "    `test 결과를 저장할 폴더 경로` 입력\n",
    "    - 나머지는 위와 동일\n",
    "    \n",
    "    \n",
    "3. 저장된 테스트 결과 확인  \n",
    "    **default_root_dir** 폴더 안에 생성되는 폴더 및 파일들\n",
    "    - **rouge_score.csv**     \n",
    "        : csv 형태로 rouge score가 출력된 파일 생성\n",
    "    - **tb_logs > default**   \n",
    "        : `version_x` 의 형태로 버전 폴더 생성   \n",
    "        : 버전 폴더 안 `events.out.tfevents.xxxxxxxxxxxx`, `hparams.yaml` 의 형태로 이벤트 파일과 하이퍼파라미터 파일 생성\n",
    "    - 이 외 파일은 생성되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=16, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='rsc/KoBART-Dacon/batch_4/not_lr_finder/num_workers4/kobart_summary-model_chp/epoch=02-val_loss=1.358.ckpt', default_root_dir='rsc/KoBART-test', deterministic=False, distributed_backend=None, enable_pl_optimizer=True, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=2, gradient_clip_val=0, hparams_file='rsc/KoBART-Dacon/batch_4/not_lr_finder/num_workers4/tb_logs/default/version_0/hparams.yaml', limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=1000, max_len=512, max_steps=None, min_epochs=1, min_steps=None, mode='test', model_path=None, move_metrics_to_cpu=False, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=10, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sync_batchnorm=False, terminate_on_nan=False, test_file='rsc/data/Validation/Dacon/dev_v1.csv', tpu_cores=<function _gpus_arg_default at 0x7efd3bf732f0>, track_grad_norm=-1, train_file='rsc/data/Training/Dacon/train_v1.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
      "using cached model\n",
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "using cached model\n",
      "using cached model\n",
      "INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=16, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='rsc/KoBART-Dacon/batch_4/not_lr_finder/num_workers4/kobart_summary-model_chp/epoch=02-val_loss=1.358.ckpt', default_root_dir='rsc/KoBART-test', deterministic=False, distributed_backend=None, enable_pl_optimizer=True, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=2, gradient_clip_val=0, hparams_file='rsc/KoBART-Dacon/batch_4/not_lr_finder/num_workers4/tb_logs/default/version_0/hparams.yaml', limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=1000, max_len=512, max_steps=None, min_epochs=1, min_steps=None, mode='test', model_path=None, move_metrics_to_cpu=False, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=10, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sync_batchnorm=False, terminate_on_nan=False, test_file='rsc/data/Validation/Dacon/dev_v1.csv', tpu_cores=<function _gpus_arg_default at 0x7f01d32501e0>, track_grad_norm=-1, train_file='rsc/data/Training/Dacon/train_v1.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
      "using cached model\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "using cached model\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "INFO:lightning:initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "using cached model\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "INFO:lightning:initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "2021-08-31 01:13:55.884760: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Testing: 0it [00:00, ?it/s]2021-08-31 01:13:59.451443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Testing:   1%|▌                                 | 4/268 [00:07<07:48,  1.78s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!python src/KoBART/train.py \\\n",
    "--train_file='rsc/Data/Training/Dacon/train_v1.csv' \\\n",
    "--test_file='rsc/Data/Validation/Dacon/dev_v1.csv' \\\n",
    "--mode='test' \\\n",
    "--checkpoint_path='rsc/By_hpram_ckpt/batch_4/not_lr_finder/kobart_summary-model_chp/epoch=02-val_loss=1.358.ckpt' \\\n",
    "--hparams_file='rsc/By_hpram_ckpt/batch_4/not_lr_finder/tb_logs/default/version_0/hparams.yaml' \\\n",
    "--batch_size=16 \\\n",
    "--num_workers=10 \\\n",
    "--gpus=2 \\\n",
    "--accelerator=ddp \\\n",
    "--default_root_dir='rsc/By_hpram_ckpt/batch_4/not_lr_finder/test_dacon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download config.json\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1H13loH6dS_2c2Z21kaBtgz42QsjkdAwO\n",
      "To: /home/tf1/kobart_summary/config.json\n",
      "100%|██████████████████████████████████████| 1.20k/1.20k [00:00<00:00, 2.95MB/s]\n",
      "Download pytorch_model.bin\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1D7BAXK_0faWW39c0ptE3FtROsVRbTNwI\n",
      "To: /home/tf1/kobart_summary/pytorch_model.bin\n",
      "496MB [00:44, 11.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "!python src/KoBART/download_binary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
